{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActiveLearning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Oao3_4n3oXEf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nowadClppbqF",
        "colab_type": "code",
        "outputId": "a783a13b-ab7f-42f2-de72-fc4222f6682f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/zalandoresearch/fashion-mnist.git"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fashion-mnist' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_xx5BrKXoXEs",
        "colab_type": "code",
        "outputId": "96577ea6-2018-4e16-ed8e-c0e0b2f36986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "######Pulls reads in MNIST data\n",
        "mnist = input_data.read_data_sets(\"fashion_mnist/\", one_hot=True)\n",
        "\n",
        "######Some options for different label functions\n",
        "######Used for different classification tasks\n",
        "\n",
        "###Here is the \"3 or not-3\" labeling function\n",
        "def find_3s(old_label):\n",
        "    if (old_label[3] == 1):\n",
        "        return [1, 0]\n",
        "    else:\n",
        "        return [0, 1]\n",
        "\n",
        "###Standard MNIST 10 digit classification labeling function\n",
        "def keep_old(old_label):\n",
        "    return old_label\n",
        "\n",
        "###Separates 0-4 and 5-9 into two classes\n",
        "def split_labels(old_label):\n",
        "    if (np.sum(old_label[:5]) == 1):\n",
        "        return [1, 0]\n",
        "    else:\n",
        "        return [0, 1]\n",
        "\n",
        "###Default is \"3 or not-3\" labeling function\n",
        "new_labels = find_3s\n",
        "\n",
        "###assign the labeling function based on 5th parameter passed to the script\n",
        "###choices are 'old' (0-9 classification), '3s' (3 or not-3 classifcation),\n",
        "###or 'split' (0-4 or 5-9 classification)\n",
        "if (len(sys.argv) >= 6):\n",
        "    if (sys.argv[5] == \"old\"):\n",
        "        new_labels = keep_old\n",
        "    elif (sys.argv[5] == \"split\"):\n",
        "        new_labels = split_labels\n",
        "    else:\n",
        "        new_labels = find_3s\n",
        "\n",
        "######Ouptut layer size determined by labeling function\n",
        "output_layer_size = len(new_labels(mnist.train.labels[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting fashion_mnist/train-images-idx3-ubyte.gz\n",
            "Extracting fashion_mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting fashion_mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting fashion_mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DBBT66qAoXEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######Here is the neural net model described in Tensor Flow MNIST example\n",
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "x  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "y_ = tf.placeholder(tf.float32, [None, output_layer_size],  name='y_')\n",
        "\n",
        "# Convolutional layer 1\n",
        "W_conv1 = weight_variable([5, 5, 1, 32])\n",
        "b_conv1 = bias_variable([32])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# Convolutional layer 2\n",
        "W_conv2 = weight_variable([5, 5, 32, 64])\n",
        "b_conv2 = bias_variable([64])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "# Fully connected layer 1\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "\n",
        "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "\n",
        "# Dropout\n",
        "keep_prob  = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "# Fully connected layer 2 (Output layer)\n",
        "W_fc2 = weight_variable([1024, output_layer_size])\n",
        "b_fc2 = bias_variable([output_layer_size])\n",
        "\n",
        "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name='y')\n",
        "\n",
        "# Evaluation functions\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Training algorithm\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8rrnnuioXE2",
        "colab_type": "code",
        "outputId": "884a6fc3-01e5-4f74-8f17-99edca4cf617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        }
      },
      "cell_type": "code",
      "source": [
        "######A function that randomly generates a value to be used when selecting examples\n",
        "######for mini-batch training when not using active learning\n",
        "def random_values(l):\n",
        "    return [random.randint(0,9) for b in l]\n",
        "\n",
        "######Ranks mnist training images according to a rank function (random for normal, \n",
        "######model evaluation for active learning)\n",
        "def choose_examples(datas = mnist.train.images, batch_size = 50, rank_function = random_values, chosen = []):\n",
        "\n",
        "    look_size = batch_size * 100\n",
        "    \n",
        "    training_lookup_index = range(0, len(mnist.train.images))\n",
        "\n",
        "    #do not look at examples that have already been seen\n",
        "    raw_remain = list(set(training_lookup_index) - set(chosen))\n",
        "    \n",
        "    #shuffle the unseen examples to randomize order\n",
        "    random.shuffle(raw_remain)\n",
        "    \n",
        "    looking_in = []\n",
        "    \n",
        "    #look at the first batch_size * 100 unseen exmaples (shuffled)\n",
        "    if (look_size >= len(raw_remain)):\n",
        "        looking_in = raw_remain\n",
        "    else:\n",
        "        looking_in = raw_remain[:look_size]\n",
        "        \n",
        "    remain_data = [datas[k] for k in looking_in]\n",
        "    \n",
        "    #rank the examples according to the rank function\n",
        "    ranks = rank_function(remain_data)\n",
        "    scores = np.column_stack((looking_in, ranks))\n",
        "    to_return = []\n",
        "\n",
        "    scores = np.array(scores)\n",
        "        \n",
        "    selected = []\n",
        "        \n",
        "    #select examples based on their scores, enough to fill a batch\n",
        "    if len(scores) >= batch_size:  \n",
        "        sort = scores[np.argsort(scores[:,1])]\n",
        "        selected = sort[:batch_size]\n",
        "    else:\n",
        "        selected = scores\n",
        "\n",
        "    #return the index value for each chosen example\n",
        "    return [int(s[0]) for s in selected]\n",
        "\n",
        "######Big mess of a function that does a lot of things\n",
        "def run_batch(runs, size, max_steps, active, extra_sample, print_every):\n",
        "    #collects information about each run\n",
        "    batch_log = []\n",
        "\n",
        "    #converts mnist test labels into new labels given label function\n",
        "    test_labels = [new_labels(b) for b in mnist.test.labels]\n",
        "\n",
        "    for i in range(runs):\n",
        "        #previously trained on examples\n",
        "        chosen = []\n",
        "\n",
        "        #order that examples were selected to be trained on\n",
        "        ordered = []\n",
        "\n",
        "        #collects information about a given run\n",
        "        run_log = []\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            #initializes the model\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            #rank function for active learning... should not be in here\n",
        "            def rank_function(examples):\n",
        "                return [abs(np.max(s) - np.min(s)) for s in sess.run(y, feed_dict={x: examples, keep_prob: 1.0})]\n",
        "\n",
        "            #rank function defaults to random, if active parameter passed\n",
        "            #then will be the active learning rank function\n",
        "            ranker = random_values\n",
        "            \n",
        "            if active:\n",
        "                ranker = rank_function\n",
        "            \n",
        "            #iterate for each step (mini-batch)\n",
        "            for step in range(max_steps):\n",
        "                #determines which train examples to look at in this mini-batch\n",
        "                next_batch = choose_examples(mnist.train.images, size, ranker, chosen)\n",
        "\n",
        "                #if re-sampling turned on, will double size of the mini-batch\n",
        "                #by randomly sampling from previously trained on exmaples\n",
        "                to_train = next_batch\n",
        "                if extra_sample and len(chosen) > 0:\n",
        "                    random.shuffle(chosen)\n",
        "                    to_train = chosen[:size] + next_batch\n",
        "\n",
        "                chosen = chosen + next_batch\n",
        "                ordered = ordered + next_batch\n",
        "\n",
        "                #if all examples have been trained on, will start over (new epoch kind of)\n",
        "                if(len(chosen) == len(mnist.train.labels)):\n",
        "                    chosen = []\n",
        "\n",
        "                #grabs images and labels based on mini-batch\n",
        "                batch_xs = [mnist.train.images[s] for s in to_train]\n",
        "                batch_ys = [mnist.train.labels[s] for s in to_train]\n",
        "\n",
        "                #counts how many positive examples were in a mini-batch\n",
        "                positive_examples = 0\n",
        "                changed_ys = [new_labels(ys) for ys in batch_ys]\n",
        "                for ys in changed_ys:\n",
        "                    positive_examples = positive_examples + ys[0]\n",
        "\n",
        "                #creates test results every print_every mini-batches\n",
        "                #passed in as a parameter\n",
        "                if (step % print_every) == 0:\n",
        "                    rr = sess.run([accuracy, cross_entropy], feed_dict={x: mnist.test.images, y_: test_labels, keep_prob: 1.0})\n",
        "                    acc = rr[0]\n",
        "                    ce = rr[1]\n",
        "                    print(acc)\n",
        "                    run_log.append([step, acc, ce, float(positive_examples)/len(to_train)])\n",
        "                \n",
        "                #trains the model using the mini-batch\n",
        "                sess.run(train_step, feed_dict={x: batch_xs, y_: changed_ys, keep_prob: 0.5})\n",
        "            \n",
        "            #after all the mini-batch training, run against test set and generate results\n",
        "            final_rr = sess.run([accuracy, cross_entropy], feed_dict={x: mnist.test.images, y_: test_labels, keep_prob: 1.0})\n",
        "            final = final_rr[0]\n",
        "            final_ce = final_rr[1]\n",
        "            print(max_steps, final)\n",
        "            run_log.append([max_steps, final, final_ce, float(positive_examples)/len(to_train)])\n",
        "            batch_log.append(run_log)\n",
        "            print(\"done with run \", i)\n",
        "\n",
        "            #start multi-epoch portion (kind of pasted on at the end)\n",
        "            epoch_logs = []\n",
        "\n",
        "            #evaluate every this many labels\n",
        "            label_range = 250\n",
        "\n",
        "            #this many epochs\n",
        "            epochs = 20\n",
        "\n",
        "            #mini-batch size\n",
        "            epoch_mini_batch_size = 50\n",
        "\n",
        "            #creates labeled data sets at label_range increments and runs multi-epoch model on that \n",
        "            for label_size in range(len(ordered)/label_range):\n",
        "                labels_length = (label_size + 1) * label_range\n",
        "                result = epoch_sample(ordered[0: labels_length], epoch_mini_batch_size, epochs, sess)\n",
        "                epoch_logs.append([labels_length, epochs, result[0], result[1]])\n",
        "            print(\"labels\\tepoch\\taccuracy\\tcross entropy\")\n",
        "            for entry in epoch_logs:\n",
        "                print(entry[0], \"\\t\", entry[1], \"\\t\", entry[2], \"\\t\", entry[3])\n",
        "    print(\"done\")\n",
        "    return batch_log\n",
        "\n",
        "######trains one model on a subset of mnist data for a certian number of epochs\n",
        "def epoch_sample(chosen, mini_batch_size, epochs, sess):\n",
        "    #initializes the model (starts it over)\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"starting epoch \", epoch)\n",
        "\n",
        "        #shuffle the labeled dataset, and do mini-batch training\n",
        "        random.shuffle(chosen)\n",
        "        batches = len(chosen) / mini_batch_size\n",
        "        for i in range(batches):\n",
        "            end = (i + 1) * mini_batch_size\n",
        "            if(end > len(chosen)):\n",
        "                end = len(chosen) - 1\n",
        "            training_data = [mnist.train.images[s] for s in chosen[i * mini_batch_size: end]]\n",
        "            training_labels = [mnist.train.labels[s] for s in chosen[i * mini_batch_size: end]]\n",
        "            sess.run(train_step, feed_dict={x: training_data, y_: [new_labels(b) for b in training_labels], keep_prob: 0.5})\n",
        "    print(\"done epoch training\")\n",
        "    epoch_rr = sess.run([accuracy, cross_entropy], feed_dict={x: mnist.test.images, y_: [new_labels(b) for b in mnist.test.labels], keep_prob: 1.0})\n",
        "    epoch_acc = epoch_rr[0]\n",
        "    epoch_ce = epoch_rr[1]\n",
        "    print(\"labels: \", len(chosen))\n",
        "    print(\"epochs: \", epochs)\n",
        "    print(\"acc: \", epoch_acc)\n",
        "    print(\"cross entropy: \", epoch_ce)\n",
        "    return epoch_rr\n",
        "\n",
        "def print_average_series(to_average, columns, column_names):\n",
        "    labels = [int(a[0]) for a in to_average[0]]\n",
        "    column_collection = []\n",
        "    for column in columns:\n",
        "        transformed = []\n",
        "        for run in to_average:\n",
        "            transformed.append([a[column] for a in run])\n",
        "        column_collection.append(np.mean(np.transpose(transformed), axis=1))\n",
        "    column_collection.insert(0, np.array(labels))\n",
        "    column_names.insert(0, 'iteration')\n",
        "    print(*column_names, sep='\\t')\n",
        "    for row in np.transpose(column_collection):\n",
        "        print(*row, sep='\\t')\n",
        "\n",
        "def print_details(runs, batch_size, iterations, active, extra_sample):\n",
        "    print(\"runs\", runs)\n",
        "    print(\"batch_size\", batch_size)\n",
        "    print(\"iterations\", iterations)\n",
        "    if(active):\n",
        "        print(\"active\")\n",
        "    else:\n",
        "        print(\"random\")\n",
        "    if(extra_sample):\n",
        "        print(\"extra sample\")\n",
        "    else:\n",
        "        print(\"standard sample\")\n",
        "    labels = [new_labels(b) for b in mnist.test.labels]\n",
        "    print(\"positive, negative\", np.sum(labels, axis=0))\n",
        "\n",
        "######not my most creative name.  does a run, and then prints out the results\n",
        "def make_a_good_test(runs, batch_size, iterations, active, extra_sample, print_every):\n",
        "    print_details(runs, batch_size, iterations, active, extra_sample)\n",
        "    results = run_batch(runs, batch_size, iterations, active, extra_sample, print_every)\n",
        "    print_details(runs, batch_size, iterations, active, extra_sample)\n",
        "    print_average_series(results, [1, 2, 3], [\"accuracy\", \"cross entropy\", \"% positive examples\"])\n",
        "\n",
        "extra_sampling = False\n",
        "print_every = 5\n",
        "\n",
        "######re-sampling defaults to False, determined by 6th parameter\n",
        "if(len(sys.argv) >= 7):\n",
        "    extra_sampling = sys.argv[6] == 'True'\n",
        "\n",
        "######test model interval defaults to 5, determined by 7th parameter\n",
        "if(len(sys.argv) >= 8):\n",
        "    print_every = int(sys.argv[7])\n",
        "\n",
        "\n",
        "######runs a test with the following parameters:\n",
        "###1st parameter: number of runs (almost always want just 1 or will be extremely long)\n",
        "###2nd parameter: how bit each mini-batch should be\n",
        "###3rd parameter: how many mini-batches in a run\n",
        "###4th parameter: active learning turned on or not\n",
        "###5th parameter: classification task ('old', '3s', 'split')\n",
        "###6th parameter: re-samping turned on or not\n",
        "###7th parameter: how often training is tested with test set\n",
        "# example: python activemnist.py 1 10 2000 True old True 5\n",
        "# would start one full run with mini-batches of size 10 for 2000 iterations with Active Learning\n",
        "# turned on, classifying digits from 0-9, with re-sampling and printing test results every 5 mini-batches\n",
        "# in addition it would then run 20 epochs at intervals of 250 label counts\n",
        "make_a_good_test(10, 16, 40,'True', extra_sampling, print_every)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "runs 10\n",
            "batch_size 16\n",
            "iterations 40\n",
            "active\n",
            "standard sample\n",
            "positive, negative [1010 8990]\n",
            "0.2905\n",
            "0.899\n",
            "0.899\n",
            "0.899\n",
            "0.899\n",
            "0.899\n",
            "0.8923\n",
            "0.6524\n",
            "40 0.9148\n",
            "done with run  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a9332493307d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;31m# turned on, classifying digits from 0-9, with re-sampling and printing test results every 5 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;31m# in addition it would then run 20 epochs at intervals of 250 label counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m \u001b[0mmake_a_good_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-a9332493307d>\u001b[0m in \u001b[0;36mmake_a_good_test\u001b[0;34m(runs, batch_size, iterations, active, extra_sample, print_every)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_a_good_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mprint_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0mprint_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mprint_average_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cross entropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"% positive examples\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a9332493307d>\u001b[0m in \u001b[0;36mrun_batch\u001b[0;34m(runs, size, max_steps, active, extra_sample, print_every)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m#creates labeled data sets at label_range increments and runs multi-epoch model on that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlabel_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mlabels_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabel_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_mini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
          ]
        }
      ]
    }
  ]
}